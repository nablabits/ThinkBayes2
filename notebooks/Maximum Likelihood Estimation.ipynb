{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[Source: Quantecon](https://python.quantecon.org/mle.html#top)\n",
    "\n",
    "## Key Ideas\n",
    "* likelihood functions are the distributions we believe generated the data we have.\n",
    "* We are maximising the parameters of the distribution that best fit the data\n",
    "* To maximise these functions we can Use the [Newton-Raphson algorithm](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) or some other methods available in the `scipy.optimization` module like `minimize` [Source](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize)\n",
    "* The key take away in Newton method is that we take the gradient and the Hessian of our probability distribution, in the case of the example, a Poisson distribution.\n",
    "* We operate on the log of the pmf as it's easier to work with it afterwards when it comes to differentiation (take a look at Linear Regressions Basics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration_k  Log-likelihood  θ                                                           \n",
      "-----------------------------------------------------------------------------------------\n",
      "0            -4.3447622      ['-1.49', '0.265', '0.244']\n",
      "1            -3.5742413      ['-3.38', '0.528', '0.474']\n",
      "2            -3.3999526      ['-5.06', '0.782', '0.702']\n",
      "3            -3.3788646      ['-5.92', '0.909', '0.82']\n",
      "4            -3.3783559      ['-6.07', '0.933', '0.843']\n",
      "5            -3.3783555      ['-6.08', '0.933', '0.843']\n",
      "Number of iterations: 6\n",
      "b_hat = ['-6.08', '0.933', '0.843']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import factorial\n",
    "\n",
    "class PoissonRegression:\n",
    "    def __init__(self, X, y, beta):\n",
    "        self.X = X\n",
    "        n, k = X.shape\n",
    "        self.y = y.reshape(n, 1)  # y as column vector.\n",
    "        self.beta = beta.reshape(k, 1)  # match features dimension\n",
    "        self.lbd = None  # init lambda\n",
    "        self.update_lambda()\n",
    "\n",
    "    def update_lambda(self):\n",
    "        # this gives us the dot product of beta against each of the vectors in X\n",
    "        # Why do we use this value for lambda? Well, it seems that if we remove the exp\n",
    "        # the algorithm won't converge.\n",
    "        self.lbd = np.exp(self.X @ self.beta)\n",
    "\n",
    "    def get_log_likelihood(self):\n",
    "        \"\"\"Compute the log-likelihood distribution (poisson).\"\"\"\n",
    "        a = self.y * np.log(self.lbd)  # ln(lbd^y), vector\n",
    "        b = - np.log(factorial(self.y))  # ln(y!), vector\n",
    "        c = - self.lbd  # ln(e^-lbd)\n",
    "        return (a + b + c).sum()\n",
    "\n",
    "    def get_gradient(self):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the log likelihood.\n",
    "\n",
    "        Take a look at Joplin notes for the worked out logic from log_likelihood and this\n",
    "        expression.\n",
    "        \"\"\"\n",
    "        return self.X.T @ (self.y - self.lbd)\n",
    "\n",
    "    def get_hessian(self):\n",
    "        \"\"\"\n",
    "        Compute the Hessian matrix of the log likelihood.\n",
    "\n",
    "        I didn't work out this out of the log likelihood but as it agrees with minimize\n",
    "        algorithm so it should be right.\n",
    "        \"\"\"\n",
    "        return -(self.X.T @ (self.lbd * self.X))\n",
    "\n",
    "    def run(self):\n",
    "        g, h = self.get_gradient(), self.get_hessian()\n",
    "        new_beta = self.beta - (np.linalg.inv(h) @ g)  # update rule\n",
    "        output = {\"error\": new_beta - self.beta}\n",
    "        self.beta = new_beta\n",
    "        self.update_lambda()  # as beta has changed\n",
    "        output[\"beta\"] = self.beta.ravel().tolist()\n",
    "        output[\"log_likelihood\"] = self.get_log_likelihood()\n",
    "        return output\n",
    "\n",
    "\n",
    "def newton_raphson(model, threshold=1e-3, max_iter=1000, display=True):\n",
    "    i = 0\n",
    "    error = 100\n",
    "\n",
    "    if display:\n",
    "        header = f'{\"Iteration_k\":<13}{\"Log-likelihood\":<16}{\"θ\":<60}'\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "    while np.any(error > threshold) and i < max_iter:\n",
    "        output = model.run()\n",
    "        beta_list = [f'{t:.3}' for t in output[\"beta\"]]\n",
    "        if display:\n",
    "            update = f'{i:<13}{output[\"log_likelihood\"]:<16.8}{beta_list}'\n",
    "            print(update)\n",
    "        i += 1\n",
    "        error = output[\"error\"]\n",
    "    print(f'Number of iterations: {i}')\n",
    "    print(f'b_hat = {beta_list}')\n",
    "\n",
    "\n",
    "X = np.array([[1, 2, 5],\n",
    "              [1, 1, 3],\n",
    "              [1, 4, 2],\n",
    "              [1, 5, 2],\n",
    "              [1, 3, 1]])\n",
    "\n",
    "y = np.array([1, 0, 1, 1, 0])\n",
    "\n",
    "# Take a guess at initial βs\n",
    "init_beta = np.array([0.1, 0.1, 0.1])\n",
    "\n",
    "# Create an object with Poisson model values\n",
    "poi = PoissonRegression(X=X, y=y, beta=init_beta)\n",
    "\n",
    "# Use newton_raphson to find the MLE\n",
    "b_hat = newton_raphson(poi, display=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "      fun: 3.37835550522458\n hess_inv: array([[29.62135324, -4.51187731, -4.23965362],\n       [-4.51187731,  0.72888415,  0.61098797],\n       [-4.23965362,  0.61098797,  0.66601047]])\n      jac: array([ 7.15255737e-07,  4.85777855e-06, -4.17232513e-07])\n  message: 'Optimization terminated successfully.'\n     nfev: 156\n      nit: 32\n     njev: 39\n   status: 0\n  success: True\n        x: array([-6.07848311,  0.93340263,  0.84329618])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution using scipy's optimize. Same, as expected\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "X = np.array([[1, 2, 5],\n",
    "              [1, 1, 3],\n",
    "              [1, 4, 2],\n",
    "              [1, 5, 2],\n",
    "              [1, 3, 1]])\n",
    "\n",
    "y = np.array([1, 0, 1, 1, 0])\n",
    "\n",
    "# Take a guess at initial βs\n",
    "init_beta = np.array([1, 1, 1])\n",
    "\n",
    "def log_likelihood_poisson(beta):\n",
    "    lbd = np.exp(X @ beta)\n",
    "    a = y * np.log(lbd)  # ln(lbd^y)\n",
    "    b = - np.log(factorial(y))  # ln(y!)\n",
    "    c = - lbd  # ln(e^-lbd)\n",
    "    return -(a + b + c).sum()\n",
    "\n",
    "res_pos = minimize(log_likelihood_poisson, x0=init_beta)\n",
    "res_pos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}